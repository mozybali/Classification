"""
Hiperparametre Optimizasyon ArayÃ¼zÃ¼
Grid Search ve Bayesian Optimization iÃ§in kullanÄ±cÄ± dostu menÃ¼
"""

import torch
from pathlib import Path
import sys

# Proje dizinini path'e ekle
sys.path.insert(0, str(Path(__file__).parent.parent))


def _configure_console() -> None:
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass


_configure_console()

from src.training.hyperparameter_optimizer import HyperparameterOptimizer, run_interactive_optimization
from src.utils.helpers import load_config
from src.preprocessing.dataloader_factory import create_dataloaders


def main():
    """Ana menÃ¼"""
    print("\n" + "="*80)
    print(" "*20 + "HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU")
    print("="*80)
    
    # Config yÃ¼kle
    config_path = 'configs/config.yaml'
    try:
        config = load_config(config_path)
        print("\nâœ“ Config baÅŸarÄ±yla yÃ¼klendi")
    except Exception as e:
        print(f"\nâŒ Config yÃ¼klenemedi: {e}")
        print("âš ï¸  VarsayÄ±lan ayarlar kullanÄ±lacak")
        config = {
            'dataset': {
                'path': 'NeAR_dataset/ALAN',
                'csv_file': 'info.csv',
                'zip_file': 'ALAN.zip'
            },
            'preprocessing': {
                'normalize': False,
                'mean': 0.0,
                'std': 1.0,
                'augmentation': {'enabled': False}
            },
            'training': {
                'batch_size': 32,
                'num_workers': 0,
                'learning_rate': 1e-4,
                'weight_decay': 1e-5,
                'optimizer': 'adam'
            },
            'model': {
                'model_type': 'resnet3d',
                'num_classes': 2,
                'in_channels': 1
            },
            'seed': 42
        }
    
    # Device kontrolÃ¼
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"âœ“ Device: {device}")
    
    if device == 'cpu':
        print("âš ï¸  GPU bulunamadÄ±, CPU kullanÄ±lacak (yavaÅŸ olabilir)")
    
    # Ana menÃ¼
    while True:
        print("\n" + "â”€"*80)
        print("ANA MENÃœ")
        print("â”€"*80)
        print("\n1. ğŸ” Grid Search Optimizasyonu")
        print("2. ğŸ¯ Bayesian Optimization (Optuna)")
        print("3. ğŸ“Š HÄ±zlÄ± Test (Az parametre, hÄ±zlÄ± sonuÃ§)")
        print("4. âš™ï¸  Ã–zel Ayarlar")
        print("5. ğŸ“– YardÄ±m ve Ã–rnekler")
        print("0. âŒ Ã‡Ä±kÄ±ÅŸ")
        
        choice = input("\nğŸ‘‰ SeÃ§iminiz (0-5): ").strip()
        
        if choice == '0':
            print("\nğŸ‘‹ Ã‡Ä±kÄ±lÄ±yor...")
            break
        
        elif choice == '1':
            grid_search_menu(config, device)
        
        elif choice == '2':
            bayesian_search_menu(config, device)
        
        elif choice == '3':
            quick_test_menu(config, device)
        
        elif choice == '4':
            custom_settings_menu(config, device)
        
        elif choice == '5':
            show_help()
        
        else:
            print("\nâŒ GeÃ§ersiz seÃ§im! LÃ¼tfen 0-5 arasÄ± bir sayÄ± girin.")


def grid_search_menu(config: dict, device: str):
    """Grid Search menÃ¼sÃ¼"""
    print("\n" + "="*80)
    print("GRID SEARCH HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU")
    print("="*80)
    
    print("\nğŸ“ Grid Search tÃ¼m parametre kombinasyonlarÄ±nÄ± sistematik olarak dener.")
    print("   Az sayÄ±da parametre seÃ§mek daha hÄ±zlÄ± sonuÃ§ verir.\n")
    
    # Parametreleri topla
    param_grid = {}
    
    # Learning Rate
    print("â”€"*80)
    print("ğŸ“Œ Learning Rate (Ã–ÄŸrenme HÄ±zÄ±)")
    print("   Ã–nerilen: 0.001, 0.0001, 0.00001")
    lr_input = input("   DeÄŸerler (virgÃ¼lle ayÄ±rÄ±n): ").strip()
    if lr_input:
        try:
            param_grid['learning_rate'] = [float(x.strip()) for x in lr_input.split(',')]
        except:
            print("   âš ï¸  GeÃ§ersiz format, varsayÄ±lan kullanÄ±lacak: [0.001, 0.0001]")
            param_grid['learning_rate'] = [0.001, 0.0001]
    else:
        param_grid['learning_rate'] = [0.0001]
    
    # Batch Size
    print("\nâ”€"*80)
    print("ğŸ“Œ Batch Size")
    print("   Ã–nerilen: 16, 32, 64 (GPU belleÄŸinize gÃ¶re)")
    bs_input = input("   DeÄŸerler (virgÃ¼lle ayÄ±rÄ±n): ").strip()
    if bs_input:
        try:
            param_grid['batch_size'] = [int(x.strip()) for x in bs_input.split(',')]
        except:
            print("   âš ï¸  GeÃ§ersiz format, varsayÄ±lan kullanÄ±lacak: [32]")
            param_grid['batch_size'] = [32]
    else:
        param_grid['batch_size'] = [32]
    
    # Optimizer
    print("\nâ”€"*80)
    print("ğŸ“Œ Optimizer")
    print("   SeÃ§enekler: adam, adamw, sgd")
    opt_input = input("   DeÄŸerler (virgÃ¼lle ayÄ±rÄ±n): ").strip()
    if opt_input:
        param_grid['optimizer'] = [x.strip().lower() for x in opt_input.split(',')]
    else:
        param_grid['optimizer'] = ['adam']
    
    # Dropout
    print("\nâ”€"*80)
    print("ğŸ“Œ Dropout (Overfitting Ã¶nleme)")
    print("   Ã–nerilen: 0.3, 0.5")
    drop_input = input("   DeÄŸerler (virgÃ¼lle ayÄ±rÄ±n, boÅŸ=atla): ").strip()
    if drop_input:
        try:
            param_grid['dropout'] = [float(x.strip()) for x in drop_input.split(',')]
        except:
            print("   âš ï¸  GeÃ§ersiz format, atlandÄ±")
    
    # Epoch sayÄ±sÄ±
    print("\nâ”€"*80)
    print("ğŸ“Œ Epoch SayÄ±sÄ± (Her kombinasyon iÃ§in)")
    print("   Ã–nerilen: 10-20 (hÄ±zlÄ± test), 50+ (gerÃ§ek eÄŸitim)")
    num_epochs = input("   Epoch sayÄ±sÄ±: ").strip()
    try:
        num_epochs = int(num_epochs)
    except:
        num_epochs = 10
        print(f"   âš ï¸  GeÃ§ersiz format, varsayÄ±lan kullanÄ±lacak: {num_epochs}")
    
    # Metrik seÃ§imi
    print("\nâ”€"*80)
    print("ğŸ“Œ Optimize Edilecek Metrik")
    print("   1. Accuracy (DoÄŸruluk)")
    print("   2. F1 Score (Dengeli metrik)")
    print("   3. AUC (ROC eÄŸrisi altÄ± alan)")
    metric_choice = input("   SeÃ§iminiz (1-3): ").strip()
    metric_map = {'1': 'accuracy', '2': 'f1', '3': 'auc'}
    metric = metric_map.get(metric_choice, 'accuracy')
    
    # Ã–zet gÃ¶ster
    print("\n" + "="*80)
    print("Ã–ZET")
    print("="*80)
    total_combinations = 1
    for key, values in param_grid.items():
        print(f"  {key}: {values}")
        total_combinations *= len(values)
    
    print(f"\n  ğŸ“Š Toplam kombinasyon: {total_combinations}")
    print(f"  ğŸ“ˆ Her kombinasyon: {num_epochs} epoch")
    print(f"  ğŸ¯ Metrik: {metric}")
    print(f"  â±ï¸  Tahmini sÃ¼re: ~{total_combinations * num_epochs * 2} dakika")
    
    # Onay
    print("\nâ”€"*80)
    confirm = input("ğŸš€ Grid Search baÅŸlatÄ±lsÄ±n mÄ±? (e/h): ").strip().lower()
    
    if confirm != 'e':
        print("âŒ Ä°ptal edildi.")
        return
    
    # Dataloaders hazÄ±rla
    print("\nğŸ“¦ Veri yÃ¼kleniyor...")
    try:
        train_loader, val_loader, _ = create_dataloaders(config)
        print("âœ“ Veri baÅŸarÄ±yla yÃ¼klendi")
    except Exception as e:
        print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
        print("âš ï¸  LÃ¼tfen dataset yolunu ve config ayarlarÄ±nÄ± kontrol edin.")
        return
    
    # Optimizer oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r
    print("\nğŸ” Grid Search baÅŸlatÄ±lÄ±yor...\n")
    
    optimizer = HyperparameterOptimizer(
        base_config=config,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device
    )
    
    results = optimizer.grid_search(
        param_grid=param_grid,
        metric=metric,
        num_epochs=num_epochs,
        save_all_models=False
    )
    
    print("\nâœ… Grid Search tamamlandÄ±!")
    print(f"ğŸ† En iyi skor: {results['best_score']:.4f}")
    print(f"ğŸ“‹ En iyi parametreler:")
    for key, value in results['best_params'].items():
        print(f"   {key}: {value}")


def bayesian_search_menu(config: dict, device: str):
    """Bayesian Search menÃ¼sÃ¼"""
    print("\n" + "="*80)
    print("BAYESIAN OPTIMIZATION (OPTUNA)")
    print("="*80)
    
    print("\nğŸ“ Bayesian Optimization parametreleri akÄ±llÄ±ca arar (Grid Search'ten hÄ±zlÄ±).")
    print("   Daha fazla deneme sayÄ±sÄ± = daha iyi sonuÃ§\n")
    
    # Deneme sayÄ±sÄ±
    print("â”€"*80)
    print("ğŸ“Œ Deneme SayÄ±sÄ±")
    print("   Ã–nerilen: 20-50 (hÄ±zlÄ±), 100+ (kapsamlÄ±)")
    n_trials = input("   Deneme sayÄ±sÄ±: ").strip()
    try:
        n_trials = int(n_trials)
    except:
        n_trials = 30
        print(f"   âš ï¸  GeÃ§ersiz format, varsayÄ±lan kullanÄ±lacak: {n_trials}")
    
    # Timeout
    print("\nâ”€"*80)
    print("ğŸ“Œ Maksimum SÃ¼re (saniye)")
    print("   BoÅŸ bÄ±rakÄ±rsanÄ±z tÃ¼m denemeler yapÄ±lÄ±r")
    timeout = input("   Timeout (saniye): ").strip()
    timeout = int(timeout) if timeout else None
    
    # Epoch sayÄ±sÄ±
    print("\nâ”€"*80)
    print("ğŸ“Œ Epoch SayÄ±sÄ± (Her deneme iÃ§in)")
    print("   Ã–nerilen: 10-20 (hÄ±zlÄ± test), 50+ (gerÃ§ek eÄŸitim)")
    num_epochs = input("   Epoch sayÄ±sÄ±: ").strip()
    try:
        num_epochs = int(num_epochs)
    except:
        num_epochs = 10
        print(f"   âš ï¸  GeÃ§ersiz format, varsayÄ±lan kullanÄ±lacak: {num_epochs}")
    
    # Parametre daÄŸÄ±lÄ±mlarÄ±
    param_distributions = {}
    
    # Learning Rate
    print("\nâ”€"*80)
    print("ğŸ“Œ Learning Rate AralÄ±ÄŸÄ± (logaritmik Ã¶lÃ§ek)")
    print("   Ã–nerilen: 0.00001 - 0.01")
    lr_min = input("   Min deÄŸer: ").strip()
    lr_max = input("   Max deÄŸer: ").strip()
    try:
        lr_min = float(lr_min) if lr_min else 1e-5
        lr_max = float(lr_max) if lr_max else 1e-2
    except:
        lr_min, lr_max = 1e-5, 1e-2
        print(f"   âš ï¸  GeÃ§ersiz format, varsayÄ±lan: [{lr_min}, {lr_max}]")
    
    param_distributions['learning_rate'] = {
        'type': 'float',
        'low': lr_min,
        'high': lr_max,
        'log': True
    }
    
    # Batch Size
    print("\nâ”€"*80)
    print("ğŸ“Œ Batch Size SeÃ§enekleri")
    print("   Ã–nerilen: 16, 32, 64")
    bs_input = input("   DeÄŸerler (virgÃ¼lle ayÄ±rÄ±n): ").strip()
    if bs_input:
        try:
            batch_sizes = [int(x.strip()) for x in bs_input.split(',')]
        except:
            batch_sizes = [16, 32, 64]
            print(f"   âš ï¸  GeÃ§ersiz format, varsayÄ±lan: {batch_sizes}")
    else:
        batch_sizes = [16, 32, 64]
    
    param_distributions['batch_size'] = {
        'type': 'categorical',
        'choices': batch_sizes
    }
    
    # Optimizer
    print("\nâ”€"*80)
    print("ğŸ“Œ Optimizer SeÃ§enekleri")
    print("   Ã–nerilen: adam, adamw")
    opt_input = input("   DeÄŸerler (virgÃ¼lle ayÄ±rÄ±n): ").strip()
    if opt_input:
        optimizers = [x.strip().lower() for x in opt_input.split(',')]
    else:
        optimizers = ['adam', 'adamw']
    
    param_distributions['optimizer'] = {
        'type': 'categorical',
        'choices': optimizers
    }
    
    # Dropout
    print("\nâ”€"*80)
    print("ğŸ“Œ Dropout AralÄ±ÄŸÄ±")
    print("   Ã–nerilen: 0.1 - 0.7")
    dr_min = input("   Min deÄŸer: ").strip()
    dr_max = input("   Max deÄŸer: ").strip()
    try:
        dr_min = float(dr_min) if dr_min else 0.1
        dr_max = float(dr_max) if dr_max else 0.7
    except:
        dr_min, dr_max = 0.1, 0.7
        print(f"   âš ï¸  GeÃ§ersiz format, varsayÄ±lan: [{dr_min}, {dr_max}]")
    
    param_distributions['dropout'] = {
        'type': 'float',
        'low': dr_min,
        'high': dr_max
    }
    
    # Metrik seÃ§imi
    print("\nâ”€"*80)
    print("ğŸ“Œ Optimize Edilecek Metrik")
    print("   1. Accuracy (DoÄŸruluk)")
    print("   2. F1 Score (Dengeli metrik)")
    print("   3. AUC (ROC eÄŸrisi altÄ± alan)")
    metric_choice = input("   SeÃ§iminiz (1-3): ").strip()
    metric_map = {'1': 'accuracy', '2': 'f1', '3': 'auc'}
    metric = metric_map.get(metric_choice, 'accuracy')
    
    # Ã–zet gÃ¶ster
    print("\n" + "="*80)
    print("Ã–ZET")
    print("="*80)
    print(f"  ğŸ“Š Deneme sayÄ±sÄ±: {n_trials}")
    print(f"  â±ï¸  Timeout: {timeout if timeout else 'Yok'} saniye")
    print(f"  ğŸ“ˆ Her deneme: {num_epochs} epoch")
    print(f"  ğŸ¯ Metrik: {metric}")
    print(f"\n  Aranacak Parametreler:")
    for key, dist in param_distributions.items():
        if dist['type'] == 'categorical':
            print(f"    {key}: {dist['choices']}")
        elif dist['type'] == 'float':
            log_str = " (log)" if dist.get('log') else ""
            print(f"    {key}: [{dist['low']}, {dist['high']}]{log_str}")
    
    print(f"\n  â±ï¸  Tahmini sÃ¼re: ~{n_trials * num_epochs * 2} dakika")
    
    # Onay
    print("\nâ”€"*80)
    confirm = input("ğŸš€ Bayesian Optimization baÅŸlatÄ±lsÄ±n mÄ±? (e/h): ").strip().lower()
    
    if confirm != 'e':
        print("âŒ Ä°ptal edildi.")
        return
    
    # Dataloaders hazÄ±rla
    print("\nğŸ“¦ Veri yÃ¼kleniyor...")
    try:
        train_loader, val_loader, _ = create_dataloaders(config)
        print("âœ“ Veri baÅŸarÄ±yla yÃ¼klendi")
    except Exception as e:
        print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
        print("âš ï¸  LÃ¼tfen dataset yolunu ve config ayarlarÄ±nÄ± kontrol edin.")
        return
    
    # Optimizer oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r
    print("\nğŸ¯ Bayesian Optimization baÅŸlatÄ±lÄ±yor...\n")
    
    optimizer = HyperparameterOptimizer(
        base_config=config,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device
    )
    
    results = optimizer.bayesian_search(
        param_distributions=param_distributions,
        n_trials=n_trials,
        metric=metric,
        num_epochs=num_epochs,
        timeout=timeout,
        n_jobs=1
    )
    
    print("\nâœ… Bayesian Optimization tamamlandÄ±!")
    print(f"ğŸ† En iyi skor: {results['best_score']:.4f}")
    print(f"ğŸ“‹ En iyi parametreler:")
    for key, value in results['best_params'].items():
        print(f"   {key}: {value}")


def quick_test_menu(config: dict, device: str):
    """HÄ±zlÄ± test menÃ¼sÃ¼"""
    print("\n" + "="*80)
    print("HIZLI TEST")
    print("="*80)
    
    print("\nğŸ“ HÄ±zlÄ± test modunda az parametre ve az epoch kullanÄ±lÄ±r.")
    print("   Sistemi test etmek ve hÄ±zlÄ± sonuÃ§ almak iÃ§in uygundur.\n")
    
    print("â”€"*80)
    print("ğŸ¯ Hangi yÃ¶ntemi denemek istersiniz?")
    print("   1. Grid Search (3-5 kombinasyon)")
    print("   2. Bayesian Search (10 deneme)")
    
    method = input("\nğŸ‘‰ SeÃ§iminiz (1-2): ").strip()
    
    if method == '1':
        # HÄ±zlÄ± Grid Search
        param_grid = {
            'learning_rate': [1e-3, 1e-4],
            'optimizer': ['adam'],
            'batch_size': [32]
        }
        num_epochs = 5
        metric = 'accuracy'
        
        print("\nğŸ“‹ HÄ±zlÄ± Grid Search AyarlarÄ±:")
        print(f"  Learning Rate: {param_grid['learning_rate']}")
        print(f"  Optimizer: {param_grid['optimizer']}")
        print(f"  Batch Size: {param_grid['batch_size']}")
        print(f"  Epoch: {num_epochs}")
        print(f"  Toplam: 2 kombinasyon Ã— {num_epochs} epoch = ~10 dakika")
        
        confirm = input("\nğŸš€ BaÅŸlat? (e/h): ").strip().lower()
        if confirm != 'e':
            return
        
        # Veri yÃ¼kle ve Ã§alÄ±ÅŸtÄ±r
        try:
            train_loader, val_loader, _ = create_dataloaders(config)
            
            optimizer = HyperparameterOptimizer(
                base_config=config,
                train_loader=train_loader,
                val_loader=val_loader,
                device=device
            )
            
            results = optimizer.grid_search(
                param_grid=param_grid,
                metric=metric,
                num_epochs=num_epochs
            )
            
            print("\nâœ… HÄ±zlÄ± test tamamlandÄ±!")
            
        except Exception as e:
            print(f"\nâŒ Hata: {e}")
    
    elif method == '2':
        # HÄ±zlÄ± Bayesian Search
        param_distributions = {
            'learning_rate': {'type': 'float', 'low': 1e-4, 'high': 1e-2, 'log': True},
            'batch_size': {'type': 'categorical', 'choices': [32]},
            'optimizer': {'type': 'categorical', 'choices': ['adam', 'adamw']},
            'dropout': {'type': 'float', 'low': 0.3, 'high': 0.6}
        }
        n_trials = 10
        num_epochs = 5
        metric = 'accuracy'
        
        print("\nğŸ“‹ HÄ±zlÄ± Bayesian Search AyarlarÄ±:")
        print(f"  Deneme sayÄ±sÄ±: {n_trials}")
        print(f"  Epoch: {num_epochs}")
        print(f"  Tahmini sÃ¼re: ~{n_trials * num_epochs * 2} dakika")
        
        confirm = input("\nğŸš€ BaÅŸlat? (e/h): ").strip().lower()
        if confirm != 'e':
            return
        
        # Veri yÃ¼kle ve Ã§alÄ±ÅŸtÄ±r
        try:
            train_loader, val_loader, _ = create_dataloaders(config)
            
            optimizer = HyperparameterOptimizer(
                base_config=config,
                train_loader=train_loader,
                val_loader=val_loader,
                device=device
            )
            
            results = optimizer.bayesian_search(
                param_distributions=param_distributions,
                n_trials=n_trials,
                metric=metric,
                num_epochs=num_epochs
            )
            
            print("\nâœ… HÄ±zlÄ± test tamamlandÄ±!")
            
        except Exception as e:
            print(f"\nâŒ Hata: {e}")


def custom_settings_menu(config: dict, device: str):
    """Ã–zel ayarlar menÃ¼sÃ¼"""
    print("\n" + "="*80)
    print("Ã–ZEL AYARLAR")
    print("="*80)
    
    print("\nâš™ï¸  Config dosyasÄ±nÄ± dÃ¼zenleyerek ayarlarÄ± deÄŸiÅŸtirebilirsiniz:")
    print(f"   ğŸ“„ configs/config.yaml")
    print("\n   Model, eÄŸitim ve dataset ayarlarÄ± bu dosyada bulunur.")
    print("   DÃ¼zenledikten sonra optimizasyonu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.")
    
    input("\nâ†µ Devam etmek iÃ§in Enter'a basÄ±n...")


def show_help():
    """YardÄ±m ve Ã¶rnekler"""
    print("\n" + "="*80)
    print("YARDIM VE Ã–RNEKLER")
    print("="*80)
    
    print("\nğŸ“š HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU NEDÄ°R?")
    print("â”€"*80)
    print("""
Hiperparametre optimizasyonu, model performansÄ±nÄ± artÄ±rmak iÃ§in en iyi
parametre kombinasyonunu bulmaktÄ±r. Optimize edebileceÄŸiniz parametreler:

  â€¢ Learning Rate: Modelin ne hÄ±zda Ã¶ÄŸreneceÄŸi
  â€¢ Batch Size: Her adÄ±mda kaÃ§ Ã¶rnek kullanÄ±lacaÄŸÄ±
  â€¢ Optimizer: Gradient descent algoritmasÄ± (Adam, AdamW, SGD)
  â€¢ Dropout: Overfitting'i Ã¶nlemek iÃ§in nÃ¶ron kapatma oranÄ±
  â€¢ Model Architecture: Katman sayÄ±sÄ±, filtre sayÄ±sÄ± vb.
""")
    
    print("\nğŸ” GRID SEARCH vs BAYESIAN SEARCH")
    print("â”€"*80)
    print("""
Grid Search:
  âœ“ TÃ¼m kombinasyonlarÄ± sistematik olarak dener
  âœ“ Basit ve anlaÅŸÄ±lÄ±r
  âœ— Ã‡ok parametre ile yavaÅŸ olabilir
  â†’ Az parametre iÃ§in ideal

Bayesian Search (Optuna):
  âœ“ AkÄ±llÄ±ca arama yapar (Ã¶nceki denemelerden Ã¶ÄŸrenir)
  âœ“ Daha az denemede iyi sonuÃ§ bulur
  âœ“ Ã‡ok parametre iÃ§in uygun
  â†’ Genel olarak Ã¶nerilen yÃ¶ntem
""")
    
    print("\nğŸ’¡ Ã–NERLER")
    print("â”€"*80)
    print("""
1. Ä°lk deneme iÃ§in HÄ±zlÄ± Test kullanÄ±n (5-10 dakika)
2. SonuÃ§lar iyiyse, daha fazla epoch ve deneme ile tekrarlayÄ±n
3. Learning rate genellikle en Ã¶nemli parametredir
4. GPU varsa daha bÃ¼yÃ¼k batch size kullanÄ±n (16, 32, 64)
5. SonuÃ§larÄ± outputs/hyperparameter_optimization/ dizininde bulabilirsiniz
""")
    
    print("\nğŸ“Š Ã–RNEK Ã‡IKTI")
    print("â”€"*80)
    print("""
Optimizasyon sonunda ÅŸunlarÄ± elde edersiniz:
  â€¢ En iyi parametreler (JSON formatÄ±nda)
  â€¢ TÃ¼m denemelerin sonuÃ§larÄ±
  â€¢ GÃ¶rselleÅŸtirmeler (Optuna iÃ§in HTML dosyalarÄ±)
  â€¢ Parameter importance analizi
  â€¢ Optimization history grafiÄŸi
""")
    
    input("\nâ†µ Ana menÃ¼ye dÃ¶nmek iÃ§in Enter'a basÄ±n...")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Program kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
    except Exception as e:
        print(f"\n\nâŒ Beklenmeyen hata: {e}")
        import traceback
        traceback.print_exc()
